import { useEffect, useRef, useState, useCallback } from 'react';
import { FaceLandmarker, FilesetResolver } from '@mediapipe/tasks-vision';

export interface PointerPosition {
  x: number;
  y: number;
  confidence: number;
  isTracking: boolean;
}

export interface GestureState {
  direction: 'none' | 'up' | 'down';
  distance: number;
  duration: number;
}

const NOSE_LANDMARK_INDEX = 1; // MediaPipeã®é¼»ãƒ©ãƒ³ãƒ‰ãƒãƒ¼ã‚¯ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

export function useNosePointer() {
  const videoRef = useRef<HTMLVideoElement | null>(null);
  const canvasRef = useRef<HTMLCanvasElement | null>(null);
  const faceLandmarkerRef = useRef<FaceLandmarker | null>(null);
  const animationFrameRef = useRef<number | null>(null);

  const [pointerPosition, setPointerPosition] = useState<PointerPosition>({
    x: 0,
    y: 0,
    confidence: 0,
    isTracking: false,
  });
  
  // processFrameå†…ã§æœ€æ–°ã®å€¤ã‚’å‚ç…§ã™ã‚‹ãŸã‚ã®Ref
  const pointerPositionRef = useRef<PointerPosition>({
    x: 0,
    y: 0,
    confidence: 0,
    isTracking: false,
  });

  const [gestureState, setGestureState] = useState<GestureState>({
    direction: 'none',
    distance: 0,
    duration: 0,
  });
  
  // gestureStateã®æ›´æ–°ã‚’æŠ‘åˆ¶ã™ã‚‹ãŸã‚ã®Ref
  const gestureStateRef = useRef<GestureState>({
    direction: 'none',
    distance: 0,
    duration: 0,
  });

  const [isInitialized, setIsInitialized] = useState(false);
  const [error, setError] = useState<string | null>(null);
  const [debugInfo, setDebugInfo] = useState<Record<string, string>>({});
  const [sensitivity, setSensitivity] = useState(2.0);
  
  // processFrameå†…ã§æœ€æ–°ã®å€¤ã‚’å‚ç…§ã™ã‚‹ãŸã‚ã®Ref
  const sensitivityRef = useRef(2.0);

  // å‰ãƒ•ãƒ¬ãƒ¼ãƒ ã®é¼»ä½ç½®ã‚’è¿½è·¡ï¼ˆã‚¸ã‚§ã‚¹ãƒãƒ£æ¤œå‡ºç”¨ï¼‰
  const prevNosePosRef = useRef<{ x: number; y: number } | null>(null);
  const gestureStartTimeRef = useRef<number | null>(null);
  const gestureStartPosRef = useRef<{ x: number; y: number } | null>(null);

  // sensitivityã®å¤‰æ›´ã‚’Refã«åæ˜ 
  useEffect(() => {
    sensitivityRef.current = sensitivity;
  }, [sensitivity]);

  // MediaPipeã®åˆæœŸåŒ–
  const initializeFaceLandmarker = useCallback(async () => {
    try {
      console.log('ğŸ”§ Initializing MediaPipe FaceLandmarker...');
      setDebugInfo((prev) => ({ ...prev, status: 'Initializing MediaPipe...' }));

      // å…¬å¼CDNãƒ‘ã‚¹ã‚’ä½¿ç”¨
      const wasmPath = 'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@latest/wasm';

      console.log(`ğŸ“¦ Loading WASM from: ${wasmPath}`);
      setDebugInfo((prev) => ({ ...prev, wasmPath }));

      const filesetResolver = await FilesetResolver.forVisionTasks(wasmPath);
      console.log('âœ… FilesetResolver created');
      setDebugInfo((prev) => ({ ...prev, filesetResolver: 'Created' }));

      console.log('ğŸ¤– Creating FaceLandmarker...');
      const landmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
        baseOptions: {
          modelAssetPath:
            'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task',
        },
        runningMode: 'VIDEO',
        numFaces: 1,
      });

      faceLandmarkerRef.current = landmarker;
      console.log('âœ… MediaPipe FaceLandmarker initialized successfully');
      setDebugInfo((prev) => ({ ...prev, status: 'MediaPipe Ready' }));
      setIsInitialized(true);
      setError(null);
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Failed to initialize MediaPipe';
      console.error('âŒ MediaPipe initialization error:', err);
      setDebugInfo((prev) => ({ ...prev, error: message }));
      setError(`MediaPipeã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: ${message}`);
    }
  }, []);

  // ãƒ“ãƒ‡ã‚ªã‚¹ãƒˆãƒªãƒ¼ãƒ ã®é–‹å§‹
  const startVideoStream = useCallback(async () => {
    try {
      console.log('ğŸ“¹ Requesting camera access...');
      setDebugInfo((prev) => ({ ...prev, camera: 'Requesting...' }));

      // ã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³å¯¾å¿œï¼šè¤‡æ•°ã®ã‚«ãƒ¡ãƒ©è¨­å®šã‚’è©¦ã™
      const constraints = [
        // ç¬¬1å„ªå…ˆï¼šãƒ•ãƒ­ãƒ³ãƒˆã‚«ãƒ¡ãƒ©ï¼ˆã‚¹ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒ³ï¼‰
        { video: { facingMode: { ideal: 'user' }, width: { ideal: 1280 }, height: { ideal: 720 } } },
        // ç¬¬2å„ªå…ˆï¼šãƒ•ãƒ­ãƒ³ãƒˆã‚«ãƒ¡ãƒ©ï¼ˆå¿…é ˆï¼‰
        { video: { facingMode: 'user' } },
        // ç¬¬3å„ªå…ˆï¼šãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚«ãƒ¡ãƒ©
        { video: true },
      ];

      let stream: MediaStream | null = null;
      let lastError: Error | null = null;

      for (const constraint of constraints) {
        try {
          console.log('ğŸ¥ Trying constraint:', constraint);
          stream = await navigator.mediaDevices.getUserMedia(constraint);
          console.log('âœ… Camera access granted with constraint:', constraint);
          setDebugInfo((prev) => ({ ...prev, camera: 'Connected' }));
          break;
        } catch (err) {
          lastError = err as Error;
          console.warn('âš ï¸ Constraint failed, trying next...', err);
        }
      }

      if (!stream) {
        throw lastError || new Error('No camera constraints worked');
      }

      if (videoRef.current) {
        videoRef.current.srcObject = stream;
        videoRef.current.muted = true; // éŸ³å£°ã‚’ç„¡åŠ¹åŒ–

        // ãƒ“ãƒ‡ã‚ªå†ç”Ÿã®æº–å‚™å®Œäº†ã‚’å¾…ã¤
        const playPromise = videoRef.current.play();
        if (playPromise !== undefined) {
          playPromise
            .then(() => {
              console.log('âœ… Video playback started');
              console.log(`ğŸ“ Video dimensions: ${videoRef.current?.videoWidth}x${videoRef.current?.videoHeight}`);
              setDebugInfo((prev) => ({
                ...prev,
                camera: 'Playing',
                videoDimensions: `${videoRef.current?.videoWidth}x${videoRef.current?.videoHeight}`,
              }));
            })
            .catch((err) => {
              console.error('âŒ Video playback error:', err);
              setDebugInfo((prev) => ({ ...prev, camera: `Error: ${err.message}` }));
            });
        }

        // ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿æ™‚ã®ãƒãƒ³ãƒ‰ãƒ©
        videoRef.current.onloadedmetadata = () => {
          console.log('âœ… Video metadata loaded');
          console.log(`ğŸ“ Video dimensions: ${videoRef.current?.videoWidth}x${videoRef.current?.videoHeight}`);
          setDebugInfo((prev) => ({
            ...prev,
            videoDimensions: `${videoRef.current?.videoWidth}x${videoRef.current?.videoHeight}`,
          }));
        };

        // ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©
        videoRef.current.onerror = (err) => {
          console.error('âŒ Video error:', err);
          setDebugInfo((prev) => ({ ...prev, camera: 'Video Error' }));
        };
      }
    } catch (err) {
      const message = err instanceof Error ? err.message : 'Failed to access camera';
      console.error('âŒ Camera access error:', err);
      setDebugInfo((prev) => ({ ...prev, camera: `Error: ${message}` }));
      setError(`ã‚«ãƒ¡ãƒ©ã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: ${message}`);
    }
  }, []);

  // ã‚¸ã‚§ã‚¹ãƒãƒ£ã®æ¤œå‡ºã¨åˆ†é¡
  const detectGesture = useCallback(
    (currentPos: { x: number; y: number }, screenHeight: number) => {
      const now = Date.now();

      if (!prevNosePosRef.current) {
        prevNosePosRef.current = currentPos;
        gestureStartTimeRef.current = now;
        gestureStartPosRef.current = currentPos;
        return;
      }

      const deltaY = currentPos.y - prevNosePosRef.current.y;
      const totalDeltaY = currentPos.y - (gestureStartPosRef.current?.y || currentPos.y);
      const duration = now - (gestureStartTimeRef.current || now);
      const distancePercent = Math.abs(totalDeltaY) / screenHeight;

      let direction: 'none' | 'up' | 'down' = 'none';

      // ã‚¸ã‚§ã‚¹ãƒãƒ£æ¤œå‡ºãƒ­ã‚¸ãƒƒã‚¯ã‚’å¾©æ´»ï¼ˆä¸‹ã‚¸ã‚§ã‚¹ãƒãƒ£ç¢ºå®šã®ãŸã‚ï¼‰
      // ä¸‹æ–¹å‘ã¸ã®ç´ æ—©ã„å‹•ãã‚’æ¤œå‡º
      if (deltaY > 2 && totalDeltaY > screenHeight * 0.02) {
        direction = 'down';
      }
      // ä¸Šæ–¹å‘ã‚¸ã‚§ã‚¹ãƒãƒ£ï¼ˆã‚­ãƒ£ãƒ³ã‚»ãƒ«æ“ä½œï¼‰
      else if (deltaY < -5 && totalDeltaY < -screenHeight * 0.05) {
        direction = 'up';
      }

      // çŠ¶æ…‹æ›´æ–°ã®æŠ‘åˆ¶ï¼šæ–¹å‘ãŒå¤‰ã‚ã£ãŸæ™‚ã®ã¿æ›´æ–°ã™ã‚‹
      // ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ã®ãŸã‚ã€æ–¹å‘ãŒåŒã˜å ´åˆã¯æ›´æ–°ã—ãªã„
      if (direction !== gestureStateRef.current.direction) {
        const newState = {
          direction,
          distance: distancePercent,
          duration,
        };
        gestureStateRef.current = newState;
        setGestureState(newState);

        // ã‚¸ã‚§ã‚¹ãƒãƒ£ãŒæ¤œå‡ºã•ã‚ŒãŸå ´åˆã€ãƒãƒ£ã‚¿ãƒªãƒ³ã‚°é˜²æ­¢ã®ãŸã‚ã«å°‘ã—å¾…ã£ã¦ã‹ã‚‰ãƒªã‚»ãƒƒãƒˆã™ã‚‹
        // ã“ã‚Œã«ã‚ˆã‚Šã€UnifiedSelectionScreenå´ã§resetGestureã‚’å‘¼ã°ãªãã¦ã‚‚è‡ªç„¶ã«ãƒªã‚»ãƒƒãƒˆã•ã‚Œã‚‹
        if (direction !== 'none') {
          setTimeout(() => {
            // ç¾åœ¨ã‚‚åŒã˜ã‚¸ã‚§ã‚¹ãƒãƒ£çŠ¶æ…‹ãªã‚‰ãƒªã‚»ãƒƒãƒˆ
            if (gestureStateRef.current.direction === direction) {
              const resetState = {
                direction: 'none' as const,
                distance: 0,
                duration: 0,
              };
              gestureStateRef.current = resetState;
              setGestureState(resetState);
              
              // æ¤œå‡ºåŸºæº–ã‚‚ãƒªã‚»ãƒƒãƒˆã—ã¦ã€é€£ç¶šæ¤œå‡ºã‚’é˜²ã
              prevNosePosRef.current = null;
              gestureStartTimeRef.current = null;
              gestureStartPosRef.current = null;
            }
          }, 500); // 500mså¾Œã«ãƒªã‚»ãƒƒãƒˆ
        }
      }

      prevNosePosRef.current = currentPos;
    },
    []
  );

  // ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ï¼ˆé¼»ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°ï¼‰
  // useCallbackã§ãƒ¡ãƒ¢åŒ–ã—ã€ä¾å­˜é…åˆ—ã‚’ç©ºã«ã™ã‚‹ã“ã¨ã§å†ç”Ÿæˆã‚’é˜²ã
  const processFrame = useCallback(() => {
    if (!videoRef.current || !faceLandmarkerRef.current) {
      animationFrameRef.current = requestAnimationFrame(processFrame);
      return;
    }

    // ãƒ“ãƒ‡ã‚ªã®æº–å‚™çŠ¶æ…‹ã‚’ç¢ºèª
    const readyState = videoRef.current.readyState;
    if (readyState !== videoRef.current.HAVE_ENOUGH_DATA) {
      // æº–å‚™ãŒã§ãã¦ã„ãªã„å ´åˆã¯ãƒ­ã‚°ã‚’æ›´æ–°ã›ãšã€æ¬¡ã®ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’å¾…ã¤
      // é »ç¹ãªçŠ¶æ…‹æ›´æ–°ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã§ã¯setDebugInfoã‚’å‘¼ã°ãªã„
      animationFrameRef.current = requestAnimationFrame(processFrame);
      return;
    }

    try {
      // é »ç¹ãªçŠ¶æ…‹æ›´æ–°ã‚’é¿ã‘ã‚‹ãŸã‚ã€ã“ã“ã§ã¯setDebugInfoã‚’å‘¼ã°ãªã„
      // setDebugInfo((prev) => ({ ...prev, videoReady: 'Ready' }));

      const results = faceLandmarkerRef.current.detectForVideo(videoRef.current, Date.now());

      if (results.faceLandmarks.length > 0) {
        const landmarks = results.faceLandmarks[0];
        const noseLandmark = landmarks[NOSE_LANDMARK_INDEX];

        if (noseLandmark) {
          const screenWidth = window.innerWidth;
          const screenHeight = window.innerHeight;

          // ãƒ“ãƒ‡ã‚ªåº§æ¨™ã‚’ã‚¹ã‚¯ãƒªãƒ¼ãƒ³åº§æ¨™ã«å¤‰æ›
          // ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®è¦æœ›ã«ã‚ˆã‚Šã€ã‚«ãƒ¡ãƒ©ãŒé¡è¡¨ç¤ºã«ãªã£ã¦ã„ã‚‹ã®ã«åˆã‚ã›ã¦å‹•ãã‚’å·¦å³åè»¢ã•ã›ã‚‹
          // æ„Ÿåº¦èª¿æ•´ã‚’è¿½åŠ : ä¸­å¿ƒ(0.5)ã‹ã‚‰ã®åå·®ã‚’å¢—å¹…ã™ã‚‹
          const currentSensitivity = sensitivityRef.current;
          const centeredX = 1 - noseLandmark.x - 0.5;
          const centeredY = noseLandmark.y - 0.5;

          const rawScreenX = (centeredX * currentSensitivity + 0.5) * screenWidth;
          const rawScreenY = (centeredY * currentSensitivity + 0.5) * screenHeight;

          // ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å‡¦ç† (Exponential Moving Average)
          // ã‚¢ãƒ«ãƒ•ã‚¡å€¤: å°ã•ã„ã»ã©æ»‘ã‚‰ã‹ã ãŒé…å»¶ãŒå¢—ãˆã‚‹ (0.1 ~ 0.5 æ¨å¥¨)
          const alpha = 0.3;

          let smoothedX = rawScreenX;
          let smoothedY = rawScreenY;

          // Refã‹ã‚‰ç¾åœ¨ã®ä½ç½®ã‚’å–å¾—
          const currentPos = pointerPositionRef.current;
          if (currentPos.isTracking) {
            smoothedX = alpha * rawScreenX + (1 - alpha) * currentPos.x;
            smoothedY = alpha * rawScreenY + (1 - alpha) * currentPos.y;
          }

          // ä¿¡é ¼åº¦ã¯æ¤œå‡ºã§ããŸæ™‚ç‚¹ã§1.0ã¨ã™ã‚‹ï¼ˆZåº§æ¨™ã¯æ·±åº¦ãªã®ã§ä¿¡é ¼åº¦ã§ã¯ãªã„ï¼‰
          const confidence = 1.0;

          const newPos = {
            x: smoothedX,
            y: smoothedY,
            confidence,
            isTracking: true,
          };

          // Refã¨Stateã®ä¸¡æ–¹ã‚’æ›´æ–°
          // ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ï¼šä½ç½®ã®å¤‰åŒ–ãŒå°ã•ã„å ´åˆã¯Stateæ›´æ–°ã‚’ã‚¹ã‚­ãƒƒãƒ—
          const prevPos = pointerPositionRef.current;
          const dist = Math.sqrt(Math.pow(newPos.x - prevPos.x, 2) + Math.pow(newPos.y - prevPos.y, 2));
          
          pointerPositionRef.current = newPos;
          
          // 1.0ãƒ”ã‚¯ã‚»ãƒ«ä»¥ä¸Šã®ç§»å‹•ã€ã¾ãŸã¯ãƒˆãƒ©ãƒƒã‚­ãƒ³ã‚°çŠ¶æ…‹ã®å¤‰åŒ–ãŒã‚ã£ãŸå ´åˆã®ã¿æ›´æ–°
          // ç„¡é™ãƒ«ãƒ¼ãƒ—é˜²æ­¢ã®ãŸã‚é–¾å€¤ã‚’ä¸Šã’ã‚‹
          if (dist > 1.0 || prevPos.isTracking !== newPos.isTracking) {
            setPointerPosition(newPos);
          }

          // ã‚¸ã‚§ã‚¹ãƒãƒ£æ¤œå‡º
          detectGesture({ x: smoothedX, y: smoothedY }, screenHeight);
        }
      } else {
        const prevPos = pointerPositionRef.current;
        if (prevPos.isTracking) {
            const newPos = { ...prevPos, isTracking: false };
            pointerPositionRef.current = newPos;
            setPointerPosition(newPos);
        }
      }
    } catch (err) {
      console.error('âŒ Frame processing error:', err);
      // ã‚¨ãƒ©ãƒ¼æ™‚ã®ã¿çŠ¶æ…‹æ›´æ–°
      setDebugInfo((prev) => ({
        ...prev,
        error: err instanceof Error ? err.message : 'Unknown error',
      }));
    }

    animationFrameRef.current = requestAnimationFrame(processFrame);
  }, [detectGesture]); // detectGestureã¯useCallbackã§ãƒ¡ãƒ¢åŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã§å®‰å…¨

  // åˆæœŸåŒ–ã¨é–‹å§‹
  useEffect(() => {
    console.log('ğŸš€ useNosePointer mounted');

    // ãƒ“ãƒ‡ã‚ªè¦ç´ ã‚’ä½œæˆï¼ˆDOMã«è¿½åŠ ã—ãªã„ã€MediaPipeã®å†…éƒ¨å‡¦ç†ç”¨ï¼‰
    if (!videoRef.current) {
      const video = document.createElement('video');
      video.autoplay = true;
      video.playsInline = true;
      video.style.display = 'none'; // éè¡¨ç¤º
      videoRef.current = video;
    }

    initializeFaceLandmarker();
    startVideoStream();

    return () => {
      console.log('ğŸ›‘ useNosePointer unmounted');
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
      if (videoRef.current?.srcObject) {
        const stream = videoRef.current.srcObject as MediaStream;
        stream.getTracks().forEach((track) => track.stop());
      }
    };
  }, []); // ç©ºã®ä¾å­˜é…åˆ—ã§ä¸€åº¦ã ã‘å®Ÿè¡Œ

  // ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†ã®é–‹å§‹
  useEffect(() => {
    if (!isInitialized) return;
    
    console.log('â–¶ï¸ Starting frame processing');
    
    // processFrameã‚’é–‹å§‹
    animationFrameRef.current = requestAnimationFrame(processFrame);

    return () => {
      if (animationFrameRef.current) {
        cancelAnimationFrame(animationFrameRef.current);
      }
    };
  }, [isInitialized, processFrame]); // processFrameã¯ãƒ¡ãƒ¢åŒ–ã•ã‚Œã¦ã„ã‚‹ã®ã§å®‰å…¨

  // ã‚¸ã‚§ã‚¹ãƒãƒ£ã®ãƒªã‚»ãƒƒãƒˆ
  const resetGesture = useCallback(() => {
    const newState = {
      direction: 'none' as const,
      distance: 0,
      duration: 0,
    };
    gestureStateRef.current = newState;
    setGestureState(newState);
    
    prevNosePosRef.current = null;
    gestureStartTimeRef.current = null;
    gestureStartPosRef.current = null;
  }, []);

  return {
    videoRef,
    canvasRef,
    pointerPosition,
    gestureState,
    isInitialized,
    error,
    resetGesture,
    debugInfo,
    sensitivity,
    setSensitivity,
  };
}
